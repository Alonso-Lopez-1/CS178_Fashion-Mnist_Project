{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Alonso-Lopez-1/CS178_Final_Project/blob/main/CS178_Final_Project.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IEKJnr5s6SXm"
      },
      "outputs": [],
      "source": [
        "import numpy as np\n",
        "import pandas as pd\n",
        "import matplotlib.pyplot as plt\n",
        "import math\n",
        "import warnings\n",
        "\n",
        "from typing import List, Tuple\n",
        "\n",
        "from sklearn.datasets import fetch_openml\n",
        "from sklearn.neighbors import KNeighborsClassifier, NearestCentroid\n",
        "from sklearn.metrics import accuracy_score, confusion_matrix, ConfusionMatrixDisplay, precision_score, recall_score, classification_report\n",
        "from sklearn.model_selection import train_test_split, cross_val_score\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.neural_network import MLPClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torchvision import datasets, transforms\n",
        "from torch.utils.data import DataLoader, random_split\n",
        "\n",
        "state = 5"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "zFJOFsph6sew"
      },
      "outputs": [],
      "source": [
        "fmnist_X, fmnist_y = fetch_openml(name='Fashion-MNIST', as_frame=False, return_X_y=True)\n",
        "fmnist_y = fmnist_y.astype(int)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "I2DFQfl56t1Y"
      },
      "outputs": [],
      "source": [
        "X_train, X_temp, y_train, y_temp = train_test_split(fmnist_X, fmnist_y, test_size=0.2, random_state=state)\n",
        "X_val, X_test, y_val, y_test = train_test_split(X_temp, y_temp, test_size=0.5, random_state=state)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5ZbkTwBs6uUY"
      },
      "outputs": [],
      "source": [
        "#------------------- K Nearest Neighbor -----------------------------\n",
        "k_values = [i for i in range(1, 11)]\n",
        "accuracy_scores = []\n",
        "cross_val_scores = []\n",
        "best_accuracy = 0\n",
        "best_k = 1\n",
        "\n",
        "for k in k_values:\n",
        "    knn = KNeighborsClassifier(n_neighbors=k, metric='euclidean') #Performing KNN for K = 0 to K = 11\n",
        "    cv_scores = cross_val_score(knn, X_train.reshape(X_train.shape[0], -1), y_train, cv=5, scoring='accuracy') #Performing cross validation\n",
        "    mean_cv_accuracy = np.mean(cv_scores) #Determining the mean accuracy\n",
        "    cross_val_scores.append(mean_cv_accuracy)\n",
        "\n",
        "    knn.fit(X_train.reshape(X_train.shape[0], -1), y_train) #Fit the model and evaluate on the validation set\n",
        "    y_val_pred = knn.predict(X_val.reshape(X_val.shape[0], -1))\n",
        "    accuracy = accuracy_score(y_val, y_val_pred)\n",
        "    accuracy_scores.append(accuracy)\n",
        "\n",
        "    if accuracy > best_accuracy:\n",
        "        best_accuracy = accuracy\n",
        "        best_k = k\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(k_values, accuracy_scores, marker='o', label='Validation Accuracy')\n",
        "plt.plot(k_values, cross_val_scores, marker='x', label='Cross-Validation Accuracy')\n",
        "plt.title('Validation Accuracy vs. Number of Neighbors (k)')\n",
        "plt.xlabel('Number of Neighbors (k)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.grid(True)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "final_knn = KNeighborsClassifier(n_neighbors=best_k, metric='euclidean') #Train final model with the best hyperparameters\n",
        "final_knn.fit(X_train.reshape(X_train.shape[0], -1), y_train)\n",
        "\n",
        "y_test_pred = final_knn.predict(X_test.reshape(X_test.shape[0], -1)) #Evaluate on the test set\n",
        "final_accuracy = accuracy_score(y_test, y_test_pred)\n",
        "final_precision = precision_score(y_test, y_test_pred, average='macro')\n",
        "final_recall = recall_score(y_test, y_test_pred, average='macro')\n",
        "\n",
        "print(f'Final Test Accuracy: {final_accuracy}')\n",
        "print(f'Final Test Precision: {final_precision}')\n",
        "print(f'Final Test Recall: {final_recall}')\n",
        "\n",
        "cm = confusion_matrix(y_test, y_test_pred)\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=[i for i in range(10)])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Final k-NN Model')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vFzyPTa_7B1l"
      },
      "outputs": [],
      "source": [
        "#------------------- Logistic Regression -----------------------------\n",
        "# Standardize the data\n",
        "scaler = StandardScaler()\n",
        "X_train = scaler.fit_transform(X_train)\n",
        "X_test = scaler.transform(X_test)\n",
        "\n",
        "c_values = np.logspace(-4, 4, 10)\n",
        "model = LogisticRegression(max_iter=1000, solver='lbfgs', multi_class='multinomial') #Create the logistic regression model\n",
        "model.fit(X_train, y_train)\n",
        "y_pred = model.predict(X_test)\n",
        "accuracy_initial = accuracy_score(y_test, y_pred)\n",
        "print(f'Accuracy: {accuracy_initial:.4f}')\n",
        "print(classification_report(y_test, y_pred)) #Generate a detailed classification report\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred) #Generating the confusion matrix\n",
        "disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=[i for i in range(10)])\n",
        "disp.plot(cmap=plt.cm.Blues)\n",
        "plt.title('Confusion Matrix for Final Logistic Regression Model')\n",
        "plt.show()\n",
        "\n",
        "accuracy_list = []\n",
        "for c in c_values:\n",
        "    model = LogisticRegression(C=c, max_iter=1000, solver='lbfgs', multi_class='multinomial') #Creating the logistic regression model with different C values\n",
        "    model.fit(X_train, y_train)\n",
        "    y_pred = model.predict(X_test)\n",
        "    accuracy = accuracy_score(y_test, y_pred)\n",
        "    accuracy_list.append(accuracy)\n",
        "\n",
        "    #print(f'\\nC={c:.4f}')\n",
        "    #print(classification_report(y_test, y_pred))\n",
        "    #conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "    #disp = ConfusionMatrixDisplay(confusion_matrix=conf_matrix, display_labels=np.unique(y_test))\n",
        "    #disp.plot(cmap=plt.cm.Blues)\n",
        "    #plt.title(f'Confusion Matrix for C={c:.4f}')\n",
        "    #plt.show()\n",
        "\n",
        "plt.figure(figsize=(8, 6)) #Accuracy vs. C values\n",
        "plt.plot(c_values, accuracy_list, marker='o')\n",
        "plt.xscale('log')\n",
        "plt.xlabel('C (Regularization strength)')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.title('Accuracy vs. C values in Logistic Regression')\n",
        "plt.grid(True)\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "aecZGmAy7CKL"
      },
      "outputs": [],
      "source": [
        "#------------------- Feed-Forward Neural Network -----------------------------\n",
        "warnings.filterwarnings('ignore')\n",
        "seed = 5 #Fixing the random seed for reproducibility\n",
        "np.random.seed(seed)\n",
        "X, y = fetch_openml('Fashion-MNIST', version=1, return_X_y=True)\n",
        "y = y.astype(int)\n",
        "\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.1, random_state=seed) #Splitting the data into training and testing sets\n",
        "scaler = StandardScaler()\n",
        "X_train_scaled = scaler.fit_transform(X_train)\n",
        "X_test_scaled = scaler.transform(X_test)\n",
        "\n",
        "def evaluate_architecture(hidden_layers, hidden_nodes): #Function that evaluates the model with different architectures\n",
        "    architecture = tuple([hidden_nodes] * hidden_layers)\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=architecture, activation='relu', solver='sgd', learning_rate_init=0.001, batch_size=256, max_iter=200, random_state=seed)\n",
        "    mlp.fit(X_train_scaled, y_train)\n",
        "    train_accuracy = mlp.score(X_train_scaled, y_train)\n",
        "    test_accuracy = mlp.score(X_test_scaled, y_test)\n",
        "    return train_accuracy, test_accuracy, mlp.loss_curve_\n",
        "\n",
        "layer_configs = [1, 2, 3]\n",
        "node_configs = [32, 64, 128]\n",
        "results = []\n",
        "\n",
        "for layers in layer_configs:\n",
        "    for nodes in node_configs:\n",
        "        train_acc, test_acc, loss_curve = evaluate_architecture(layers, nodes)\n",
        "        results.append((layers, nodes, train_acc, test_acc, loss_curve))\n",
        "\n",
        "plt.figure(figsize=(15, 10))\n",
        "for layers in layer_configs:\n",
        "    for nodes in node_configs:\n",
        "        for result in results:\n",
        "            if result[0] == layers and result[1] == nodes:\n",
        "                plt.plot(result[4], label=f'{layers} Layers, {nodes} Nodes')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Loss Curves for Different Network Architectures')\n",
        "plt.show()\n",
        "\n",
        "train_accuracies = []\n",
        "test_accuracies = []\n",
        "labels = []\n",
        "\n",
        "for result in results:\n",
        "    train_accuracies.append(result[2])\n",
        "    test_accuracies.append(result[3])\n",
        "    labels.append(f'{result[0]} Layers, {result[1]} Nodes')\n",
        "\n",
        "x = np.arange(len(labels))\n",
        "\n",
        "plt.figure(figsize=(15, 6))\n",
        "plt.bar(x - 0.2, train_accuracies, 0.4, label='Training Accuracy')\n",
        "plt.bar(x + 0.2, test_accuracies, 0.4, label='Testing Accuracy')\n",
        "plt.xticks(x, labels, rotation=45)\n",
        "plt.xlabel('Network Architecture')\n",
        "plt.ylabel('Accuracy')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Accuracy for Different Network Architectures')\n",
        "plt.show()\n",
        "\n",
        "best_result = max(results, key=lambda x: x[3])  # Selecting based on test accuracy\n",
        "best_layers, best_nodes = best_result[0], best_result[1]\n",
        "best_architecture = tuple([best_nodes] * best_layers)\n",
        "best_mlp = MLPClassifier(hidden_layer_sizes=best_architecture, activation='relu', solver='sgd', learning_rate_init=0.001, batch_size=256, max_iter=200, random_state=seed)\n",
        "best_mlp.fit(X_train_scaled, y_train)\n",
        "y_pred = best_mlp.predict(X_test_scaled)\n",
        "\n",
        "conf_matrix = confusion_matrix(y_test, y_pred)\n",
        "disp = ConfusionMatrixDisplay(conf_matrix, display_labels=best_mlp.classes_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "disp.plot(cmap=plt.cm.Blues, values_format='d')\n",
        "plt.title('Confusion Matrix for Best Architecture')\n",
        "plt.show()\n",
        "\n",
        "train_sizes = [50, 500, 2000, 5000, 10000, 20000, 40000]\n",
        "train_errors = []\n",
        "test_errors = []\n",
        "\n",
        "for size in train_sizes:\n",
        "    X_train_subset = X_train_scaled[:size]\n",
        "    y_train_subset = y_train[:size]\n",
        "    best_mlp.fit(X_train_subset, y_train_subset)\n",
        "    train_errors.append(1 - best_mlp.score(X_train_subset, y_train_subset))\n",
        "    test_errors.append(1 - best_mlp.score(X_test_scaled, y_test))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_errors, label='Training Error', marker='o')\n",
        "plt.plot(train_sizes, test_errors, label='Testing Error', marker='x')\n",
        "plt.xlabel('Number of Training Data Points')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Error Rates for Different Training Sizes')\n",
        "plt.show()\n",
        "\n",
        "learning_rates = [0.0005, 0.001, 0.005, 0.01] #Plotting learning curves for different learning rates\n",
        "loss_curves = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    mlp = MLPClassifier(hidden_layer_sizes=(64,), activation='relu', solver='sgd', learning_rate_init=lr, batch_size=256, max_iter=100, random_state=seed)\n",
        "    mlp.fit(X_train_scaled[:10000], y_train[:10000])\n",
        "    loss_curves.append(mlp.loss_curve_)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(loss_curves[i], label=f'lr={lr}')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Loss Curves for Different Learning Rates')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "c1xB_UnG7CsH"
      },
      "outputs": [],
      "source": [
        "#------------------- Convolutional Neural Network -----------------------------\n",
        "seed = 5\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class NumpyDataset(Dataset): #Dataset class that handles NumPy arrays\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index].reshape(28, 28).astype(np.float32) #Reshape to 2D image and convert to float32\n",
        "        y = self.targets[index].astype(np.int64) #Convert labels to int64\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "scaler = StandardScaler()\n",
        "fmnist_X = scaler.fit_transform(fmnist_X)\n",
        "\n",
        "# Define transformations\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = NumpyDataset(X_train, y_train, transform=transform)\n",
        "val_dataset = NumpyDataset(X_val, y_val, transform=transform)\n",
        "test_dataset = NumpyDataset(X_test, y_test, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "class CNN(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "def initialize_model():\n",
        "    model = CNN()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    return model, criterion, optimizer\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "kf = KFold(n_splits=5, shuffle=True, random_state=seed)\n",
        "cv_train_accuracies = []\n",
        "cv_val_accuracies = []\n",
        "\n",
        "for train_index, val_index in kf.split(train_dataset):\n",
        "    train_subset = Subset(train_dataset, train_index)\n",
        "    val_subset = Subset(train_dataset, val_index)\n",
        "\n",
        "    train_loader = DataLoader(train_subset, batch_size=batch_size, shuffle=True)\n",
        "    val_loader = DataLoader(val_subset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "    model, criterion, optimizer = initialize_model()\n",
        "    train(model, train_loader, criterion, optimizer, epochs=10)\n",
        "\n",
        "    train_accuracy = evaluate(model, train_loader)\n",
        "    val_accuracy = evaluate(model, val_loader)\n",
        "    cv_train_accuracies.append(train_accuracy)\n",
        "    cv_val_accuracies.append(val_accuracy)\n",
        "\n",
        "    print(f'Fold train accuracy: {train_accuracy}, Fold validation accuracy: {val_accuracy}')\n",
        "\n",
        "print(f'Mean cross-validation train accuracy: {np.mean(cv_train_accuracies)}')\n",
        "print(f'Mean cross-validation validation accuracy: {np.mean(cv_val_accuracies)}')\n",
        "\n",
        "model, criterion, optimizer = initialize_model()\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "train(model, train_loader, criterion, optimizer, epochs=10)\n",
        "test_accuracy = evaluate(model, test_loader)\n",
        "print(f'Test accuracy: {test_accuracy}')\n",
        "\n",
        "train_sizes = [50, 500, 2000, 5000, 10000, 20000, 40000]\n",
        "train_errors = []\n",
        "val_errors = []\n",
        "\n",
        "for size in train_sizes:\n",
        "    subset_train_loader = DataLoader(random_split(train_dataset, [size, len(train_dataset) - size])[0], batch_size=batch_size, shuffle=True)\n",
        "    model, criterion, optimizer = initialize_model()\n",
        "    train(model, subset_train_loader, criterion, optimizer, epochs=10)\n",
        "    train_errors.append(1 - evaluate(model, subset_train_loader))\n",
        "    val_errors.append(1 - evaluate(model, val_loader))\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "plt.plot(train_sizes, train_errors, label='Training Error', marker='o')\n",
        "plt.plot(train_sizes, val_errors, label='Validation Error', marker='x')\n",
        "plt.xlabel('Number of Training Data Points')\n",
        "plt.ylabel('Error Rate')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Error Rates for Different Training Sizes')\n",
        "plt.show()\n",
        "\n",
        "learning_rates = [0.0005, 0.001, 0.005, 0.01]\n",
        "loss_curves = []\n",
        "\n",
        "for lr in learning_rates:\n",
        "    model, criterion, optimizer = initialize_model()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=lr, momentum=0.9)\n",
        "    losses = []\n",
        "    for epoch in range(10):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        losses.append(epoch_loss)\n",
        "    loss_curves.append(losses)\n",
        "\n",
        "plt.figure(figsize=(10, 6))\n",
        "for i, lr in enumerate(learning_rates):\n",
        "    plt.plot(loss_curves[i], label=f'lr={lr}')\n",
        "plt.xlabel('Epochs')\n",
        "plt.ylabel('Loss')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.title('Loss Curves for Different Learning Rates')\n",
        "plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2vhez-yN7Zf2"
      },
      "outputs": [],
      "source": [
        "#------------------- Convolutional Neural Network -----------------------------\n",
        "seed = 5\n",
        "np.random.seed(seed)\n",
        "torch.manual_seed(seed)\n",
        "\n",
        "class NumpyDataset(Dataset):\n",
        "    def __init__(self, data, targets, transform=None):\n",
        "        self.data = data\n",
        "        self.targets = targets\n",
        "        self.transform = transform\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.data)\n",
        "\n",
        "    def __getitem__(self, index):\n",
        "        x = self.data[index].reshape(28, 28).astype(np.float32)  # Reshape to 2D image and convert to float32\n",
        "        y = self.targets[index].astype(np.int64)  # Convert labels to int64\n",
        "        if self.transform:\n",
        "            x = self.transform(x)\n",
        "        return x, y\n",
        "\n",
        "scaler = StandardScaler()\n",
        "fmnist_X = scaler.fit_transform(fmnist_X)\n",
        "\n",
        "transform = transforms.Compose([\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize((0.5,), (0.5,))\n",
        "])\n",
        "\n",
        "train_dataset = NumpyDataset(X_train, y_train, transform=transform)\n",
        "val_dataset = NumpyDataset(X_val, y_val, transform=transform)\n",
        "test_dataset = NumpyDataset(X_test, y_test, transform=transform)\n",
        "\n",
        "batch_size = 256\n",
        "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
        "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
        "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
        "\n",
        "#Defining different CNN architectures\n",
        "class CNN1(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN1, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(32 * 7 * 7, 128)\n",
        "        self.fc2 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 32 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.fc2(x)\n",
        "        return x\n",
        "\n",
        "class CNN2(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN2, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 64 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class CNN3(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN3, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 64, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 7 * 7, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = x.view(-1, 128 * 7 * 7)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class CNN4(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN4, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 32, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(128 * 3 * 3, 256)\n",
        "        self.fc2 = nn.Linear(256, 128)\n",
        "        self.fc3 = nn.Linear(128, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 128 * 3 * 3)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "class CNN5(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(CNN5, self).__init__()\n",
        "        self.conv1 = nn.Conv2d(1, 16, kernel_size=3, padding=1)\n",
        "        self.conv2 = nn.Conv2d(16, 32, kernel_size=3, padding=1)\n",
        "        self.conv3 = nn.Conv2d(32, 64, kernel_size=3, padding=1)\n",
        "        self.pool = nn.MaxPool2d(kernel_size=2, stride=2, padding=0)\n",
        "        self.fc1 = nn.Linear(64 * 3 * 3, 128)\n",
        "        self.fc2 = nn.Linear(128, 64)\n",
        "        self.fc3 = nn.Linear(64, 10)\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x.view(-1, 1, 28, 28)\n",
        "        x = self.pool(self.relu(self.conv1(x)))\n",
        "        x = self.pool(self.relu(self.conv2(x)))\n",
        "        x = self.pool(self.relu(self.conv3(x)))\n",
        "        x = x.view(-1, 64 * 3 * 3)\n",
        "        x = self.relu(self.fc1(x))\n",
        "        x = self.relu(self.fc2(x))\n",
        "        x = self.fc3(x)\n",
        "        return x\n",
        "\n",
        "def initialize_model(model_class):\n",
        "    model = model_class()\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(model.parameters(), lr=0.001, momentum=0.9)\n",
        "    return model, criterion, optimizer\n",
        "\n",
        "def train(model, train_loader, criterion, optimizer, epochs=10):\n",
        "    model.train()\n",
        "    for epoch in range(epochs):\n",
        "        running_loss = 0.0\n",
        "        for inputs, labels in train_loader:\n",
        "            optimizer.zero_grad()\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            running_loss += loss.item() * inputs.size(0)\n",
        "        epoch_loss = running_loss / len(train_loader.dataset)\n",
        "        print(f'Epoch {epoch+1}/{epochs}, Loss: {epoch_loss:.4f}')\n",
        "\n",
        "def evaluate(model, data_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    with torch.no_grad():\n",
        "        for inputs, labels in data_loader:\n",
        "            outputs = model(inputs)\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "    accuracy = correct / total\n",
        "    return accuracy\n",
        "\n",
        "cnn_models = [CNN1, CNN2, CNN3, CNN4, CNN5]\n",
        "\n",
        "for i, model_class in enumerate(cnn_models):\n",
        "    print(f'\\nEvaluating model {i+1}/{len(cnn_models)}')\n",
        "    model, criterion, optimizer = initialize_model(model_class)\n",
        "    train(model, train_loader, criterion, optimizer, epochs=10)\n",
        "    test_accuracy = evaluate(model, test_loader)\n",
        "    print(f'Model {i+1} test accuracy: {test_accuracy:.4f}')\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyP2JLwdvrcJukyXTn3ntCQ2",
      "include_colab_link": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}